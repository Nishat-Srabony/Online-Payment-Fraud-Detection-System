# -*- coding: utf-8 -*-
"""Group 8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZXd1dc6X2lqq-ajYPy5otlKZlItFA0kT
"""

import pandas as pd
import numpy as np
import random
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.datasets import load_wine
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

dataset = pd.read_csv("/content/drive/MyDrive/CSE422/Project of 422/fraudTrain.csv.zip")

dataset.head()

from google.colab import drive
drive.mount('/content/drive')

dataset.shape

dataset.keys()           # Column name & data type

dataset.isnull().sum()

#  feature er moddhey kono nul value chilo na tai null value create korar jonno eta korechi

for i in range (len(dataset)) :

  random_num = random.randint (1,1000)        #  random integer value between the lower and higher limits.

  if random_num == 1:
    dataset.loc [i,'amt'] = np.nan              # Pandas dataframe is a modified version of dictionary.

dataset.isnull().sum()        # True for NULL values otherwise False

dataset['dummy'] = [np.nan] * len(dataset)     # adding a dummy column with a nan values

dataset.head()

dataset.isnull().sum()

dataset.to_csv('modified_dataset.csv',index=False)



dataset1= pd.read_csv("modified_dataset.csv")
dataset1.head(10)

dataset1.isnull().sum()

if "dummy" in dataset1.columns:
  dataset1 = dataset.drop(["dummy"],axis=1)

dataset1.isnull().sum()

impute = SimpleImputer (missing_values=np.nan, strategy ="mean")
impute.fit(dataset1[['amt']])                     #  train korche and typically represented as a 2D array or matrix
dataset1['amt']= impute.transform(dataset1[['amt']])
dataset1['amt']

dataset1.isnull().sum()

dataset1.info()

dataset1.is_fraud.sum()

dataset1.keys()

enc=LabelEncoder()        # LabelEncoder() er maddhome categorical theke numerical e convert korechi
dataset1['merchant_enc']= enc.fit_transform(dataset1['merchant'])
dataset1[['merchant','merchant_enc']]

enc=LabelEncoder()
dataset1['category_enc']= enc.fit_transform(dataset1['category'])
dataset1[['category','category_enc']]

enc=LabelEncoder()
dataset1['first_enc']= enc.fit_transform(dataset1['first'])
dataset1[['first','first_enc']]

enc=LabelEncoder()
dataset1['last_enc']= enc.fit_transform(dataset1['last'])
dataset1[['last','last_enc']]

enc=LabelEncoder()
dataset1['gender_enc']= enc.fit_transform(dataset1['gender'])
dataset1[['gender','gender_enc']]

enc=LabelEncoder()
dataset1['street_enc']= enc.fit_transform(dataset1['street'])
dataset1[['street','street_enc']]

enc=LabelEncoder()
dataset1['city_enc']= enc.fit_transform(dataset1['city'])
dataset1[['city','city_enc']]

enc=LabelEncoder()
dataset1['state_enc']= enc.fit_transform(dataset1['state'])
dataset1[['state','state_enc']]

enc=LabelEncoder()
dataset1['job_enc']= enc.fit_transform(dataset1['job'])
dataset1[['job','job_enc']]

enc=LabelEncoder()
dataset1['dob_enc']= enc.fit_transform(dataset1['dob'])
dataset1[['dob','dob_enc']]

enc=LabelEncoder()
dataset1['trans_num_enc']= enc.fit_transform(dataset1['trans_num'])
dataset1[['trans_num','trans_num_enc']]

# fraud detection ta kon time and kon month e hocche seta mainly deteche korbe

dataset1[['trans_date_trans_time']]

dataset1['trans_date_trans_time']= pd.to_datetime(dataset1['trans_date_trans_time'])

dataset1['Month'] = dataset1['trans_date_trans_time'].dt.month

dataset1[['Month']]

#  Now we get pure numerical dataset

# porobortite dataset er moddhey corelation and feature scalling , spliting ,model test and traing e model prediction label ta ber korbo

dataset1.keys()

encoded_dataset= dataset1 [['cc_num','amt','zip','lat','long','city_pop','dob_enc','trans_num_enc','unix_time','merch_lat','merch_long','is_fraud','merchant_enc','category_enc','first_enc','last_enc','gender_enc','street_enc','city_enc','state_enc','job_enc','Month']]

# feature scalling er importent part holo data set er moddhey bivinno corelation create kora jar karone corelation matrix create korchi jetar moddhey 1 means perfect positive corelation,  -1 means perfect negetive corelation, 0 means no corelation

data_corr= encoded_dataset.corr()        # feature scalling for correlation, matrix corelation(1,-1,0)
data_corr

# corelation matrix visulaization korar jonno heatmap create korechi

heatmap_corr= sns.heatmap (data_corr,cmap='YlGnBu')   #  Heatmap are used to visualize data patterns.
heatmap_corr

# jei dataset highly corelated thake segulo remove kore dicchi

# Removing zip, long, merch_long..... Colomns

encoded_dataset3 = dataset1[['cc_num', 'amt', 'lat', 'city_pop','dob','trans_num','unix_time','is_fraud','merch_lat', 'merchant_enc', 'category_enc','first_enc', 'last_enc', 'gender_enc', 'street_enc', 'city_enc', 'state_enc', 'job_enc', 'Month']]
encoded_dataset3

data_corr2= encoded_dataset3.corr(numeric_only=True)
data_corr2

heatmap_corr2= sns.heatmap (data_corr2,cmap='YlGnBu')   #  Heatmap are used to visualize data patterns.
heatmap_corr2

# 2 ta feature highly corelated thake tahole amder feature theke jekono ekta feature drop kore dite pari

# Removing only marchant_lat
encoded_dataset4= dataset1[['cc_num', 'amt', 'lat', 'city_pop', 'unix_time', 'is_fraud', 'merchant_enc','category_enc', 'first_enc', 'last_enc', 'gender_enc', 'street_enc', 'city_enc', 'state_enc','job_enc', 'Month']]
encoded_dataset4

data_corr3 =encoded_dataset4.corr()
data_corr3

heatmap_corr3= sns.heatmap(data_corr3, cmap="YlGnBu")
heatmap_corr3

x_train, x_test, y_train, y_test= train_test_split(dataset1[['cc_num','amt','lat','city_pop','unix_time', 'merchant_enc', 'category_enc', 'first_enc','last_enc','gender_enc','street_enc','city_enc','state_enc','job_enc','Month' ]], dataset1['is_fraud'], test_size=0.30, random_state=0)

#  test_size working 30% test and 70% train. random_state working initially random splitting the data into train and test

x_train

x_test

y_train

y_test.shape

# scalling --> dataset er fratures gulo bivinno unit abong magnitute e calculate kora thake jetar moddhey huge diffrence create kore  tachara diffrent distance based algo ache jemon knn tader moddhey scalling down importing factor.
# eikhane 2 type scalling use korchi ekta minmaxscaler--> normalization jekhane amra features guloke  scale down korchi   and knn er moddhey scalling important

sacler= MinMaxScaler()       # features guloke  scale down korchi , normalization
sacler.fit(x_train)
x_train_minmaxcaler= sacler.transform(x_train)
x_train_minmaxcaler

sacler= MinMaxScaler()
sacler.fit(x_test)
x_test_minmaxcaler= sacler.transform(x_test)
x_test_minmaxcaler

x_test_minmaxcaler.shape

y_train

# 2nd type StandardScaler--> standardization jeta scale down the features based on standard normal distribution

sacler2= StandardScaler()    # to compare variables with different units or scales,value will be approximately 1 ,standardization
sacler2.fit(x_train)
x_train_StandardScaler= sacler2.transform(x_train)
x_train_StandardScaler

"""**KNN**

"""

from sklearn.neighbors import KNeighborsClassifier

knn= KNeighborsClassifier()
knn.fit(x_train_minmaxcaler,y_train)        # learning

y_predict= knn.predict(x_test_minmaxcaler)
y_predict

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_predict)
print('Accuracy = %f' % (precision) + '%')


recall = recall_score(y_test, y_predict)
print('Accuracy = %f' % (recall) + '%')


f1 = f1_score(y_test, y_predict)
print('Accuracy = %f' % (f1) + '%')

# predict lebel and actual lebel er moddhey comparison korbo and accuracy ta check korchi




# # why accuracy 99%?
#  accuracy is showing as 99%, it means that 99% of the predictions were correct, and only 1% of them were incorrect. This suggests that the model is performing very well on the test data.

counter=0

for i in range (len(y_test)):
  if y_predict[i] != y_test.iloc[i] :  # iloc is a library it selecting and manipulating data in DataFrames and Series
            counter+=1



# counter--> (the number of errors) , total number of predictions---> (len(y_predict)
# error--> incorrect prediction

error = counter / len(y_predict)
print('Error = %f' % (error*100) + '%')

knn_accuracy_withoutundersample = (1-error)      # find out the correct prediction
print('Accuracy = %f' % (knn_accuracy_withoutundersample*100) + '%')

# array(truepositive,  falsepositive   )
#     ( falsenegetive, truenegetive)

# confusion_matrix--> error number find out korchi, classification model er performance discribe kore

knn_cm = confusion_matrix(y_test,y_predict)    # model performance, find out error number describe classification model
knn_cm

# map theke 1st line actually fraud kintu amr model o fraud detect korche, actually not fraud kinktu amr model fraud detect koreche .2nd part actually fraud kintu amr model not fraud detect korche ,actually not fraud kintu amr model o not fraud detect korche

sns.heatmap(knn_cm, annot=True, cmap='Pastel1_r',xticklabels=['Fraud','Not Fraud'], yticklabels=['Fraud',"Not Fraud"])

# annot = True --> neumeric value te show korche. xticklabels --> x axis e label show korteche

"""**Logistic Regression**

"""

logistic_reg = LogisticRegression()
logistic_reg.fit(x_train_minmaxcaler,y_train)  # training data

y_predict1= logistic_reg.predict(x_test_minmaxcaler)      # label predict korche

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_predict1)
print('precision = %f' % (precision) + '%')


recall = recall_score(y_test, y_predict1)
print('recall = %f' % (recall) + '%')


f1 = f1_score(y_test, y_predict1)
print('f1 = %f' % (f1)  + '%')

counter1=0

for i in range ( len(y_test) ):
  if y_predict1[i] != y_test.iloc[i] :  # iloc is a library it selecting and manipulating data in DataFrames and Series
            counter1+=1


error1 = counter1 / len(y_predict1)
print('Error = %f' % (error1*100) + '%')

logreg_accuracy_withoutundersample = (1-error1)
print('Accuracy = %f' % (logreg_accuracy_withoutundersample*100) + '%')

logreg_cm = confusion_matrix(y_test,y_predict1)
logreg_cm

sns.heatmap(logreg_cm, annot=True, cmap='Pastel1_r',xticklabels=['Fraud','Not Fraud'],yticklabels=['Fraud','Not Fraud'] )

"""**Naive Bayes**"""

naive_bayes = GaussianNB()
naive_bayes.fit(x_train_minmaxcaler,y_train)

y_predict2= naive_bayes.predict(x_test_minmaxcaler)

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_predict2)
print('precision = %f' % (precision) + '%')


recall = recall_score(y_test, y_predict2)
print('recall = %f' % (recall) + '%')


f1 = f1_score(y_test, y_predict2)
print('f1 = %f' % (f1) + '%')

counter2=0

for i in range ( len(y_test) ):
  if y_predict2[i] != y_test.iloc[i] :  # iloc is a library it selecting and manipulating data in DataFrames and Series
            counter2+=1


error2 = counter2 / len(y_predict2)
print('Error = %f' % (error2*100) + '%')

naivebayes_accuracy_withoutundersample = (1-error2)
print('Accuracy = %f' % (naivebayes_accuracy_withoutundersample*100) + '%')

naive_bayes_cm = confusion_matrix(y_test,y_predict2)
naive_bayes_cm

sns.heatmap(naive_bayes_cm , annot=True, cmap='Pastel1_r',xticklabels=['Fraud','Not Fraud'],yticklabels=['Fraud','Not Fraud'] )

# Under Sampling---> imbalance dataset ke balance dataset banacchi

"""**Under Sampling**"""

updated_dataset= dataset1[['cc_num','amt','lat','city_pop','unix_time','merchant_enc','category_enc','first_enc','last_enc','gender_enc','street_enc','city_enc','state_enc','job_enc','Month','is_fraud']]


fraud= updated_dataset[dataset1.is_fraud == 1]

not_fraud= updated_dataset[dataset1.is_fraud == 0]

# use to extract more accurate information from originally imbalanced datasets. Take a random sample of non fraud rows.

sampled_not_fraud= not_fraud.sample (n=fraud.shape[0])    # fraud joto length e chilo not fraud totogulo anteche

updated_dataset= pd.concat([fraud,sampled_not_fraud])

updated_dataset

print(fraud.shape)

print(updated_dataset.shape)

# balance dataset ke train and test korchi
x_train,x_test,y_train,y_test = train_test_split(updated_dataset[['cc_num', 'amt', 'lat', 'city_pop', 'unix_time', 'merchant_enc','category_enc', 'first_enc', 'last_enc', 'gender_enc', 'street_enc', 'city_enc', 'state_enc','job_enc', 'Month']], updated_dataset['is_fraud'], test_size=0.30, random_state=0)

x_train

scaler= MinMaxScaler()
scaler.fit(x_train)

x_train_minmaxcaler1= scaler.transform(x_train)

x_train_minmaxcaler1

x_test

scaler= MinMaxScaler()
scaler.fit(x_test)

x_test_minmaxcaler1= scaler.transform(x_test)

x_test_minmaxcaler1

# KNN Model

knn= KNeighborsClassifier()
knn.fit(x_train_minmaxcaler1,y_train)

y_predict3= knn.predict(x_test_minmaxcaler1)
y_predict3

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_predict3)
print('precision = %f' % (precision) + '%')


recall = recall_score(y_test, y_predict3)
print('recall = %f' % (recall) + '%')


f1 = f1_score(y_test, y_predict3)
print('f1 = %f' % (f1)  + '%')

# under sampling korar por accuracy kome geche

counter3=0

for i in range ( len(y_test) ):
  if y_predict3[i] != y_test.iloc[i] :  # iloc is a library it selecting and manipulating data in DataFrames and Series
            counter3+=1


error3 = counter3 / len(y_predict3)
print('Error = %f' % (error3*100) + '%')

knn_accuracy1 = (1-error3)
print('Accuracy = %f' % (knn_accuracy1*100) + '%')

knn_confuMat = confusion_matrix(y_test, y_predict3)
knn_confuMat

# under sampling korar por accuracy kome geche and not fraud and fraud 2tai balanced hoye geche but before sampling er somoy balanced hoyni
# naive,logis reg ,knn e balance create hoyeche

sns.heatmap(knn_confuMat, annot= True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Not Fraud'],yticklabels=['Fraud' ,'Not Fraud'] )

# logistic regression

logreg = LogisticRegression()
logreg.fit(x_train_minmaxcaler1,y_train)

y_predict4 = logreg .predict(x_test_minmaxcaler1)
y_predict4

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_predict4)
print('precision = %f' % (precision) + '%')


recall = recall_score(y_test, y_predict4)
print('recall = %f' % (recall) + '%')


f1 = f1_score(y_test, y_predict4)
print('f1 = %f' % (f1)  + '%')

# logistic reg

counter4 = 0

for i in range( len(y_test) ):
    if y_predict4[i] != y_test.iloc[i]:
        counter4 += 1

error4 = counter4/len(y_predict4)
print( "Error = %f " % (error4*100) + '%' )

logreg_accuracy = (1-error4)
print( "Accuracy = %f " % (logreg_accuracy*100) + '%' )

logreg_confuMat= confusion_matrix(y_test ,y_predict4)
logreg_confuMat

sns.heatmap(logreg_confuMat, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Not Fraud'],yticklabels=['Fraud' ,'Not Fraud'])

#  Naive Bayes

naive_bayes= GaussianNB()
naive_bayes.fit(x_train_minmaxcaler1,y_train)

y_predt5 = naive_bayes.predict(x_test_minmaxcaler1)
y_predt5

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_predt5)
print('precision = %f' % (precision) + '%')


recall = recall_score(y_test, y_predt5)
print('recall = %f' % (recall) + '%')


f1 = f1_score(y_test, y_predt5)
print('f1 = %f' % (f1)  + '%')

count5 = 0

for i in range(len(y_test)):
    if y_predt5[i] != y_test.iloc[i]:
       count5 = count5 + 1

error5 = count5/len(y_predt5)
print( "Error = %f " % (error5*100) + '%' )
nb_accuracy1 = (1-error5)
print( "Accuracy = %f " % (nb_accuracy1*100) + '%' )

nb_confuMat= confusion_matrix(y_test ,y_predt5)
nb_confuMat

sns.heatmap(nb_confuMat, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Not Fraud'],yticklabels=['Fraud' ,'Not Fraud'])



"""**Comparison**"""

# Comparison korchi model gulo

# WithOut Undersampling

accuracy = {'Model': ['KNN', 'LR', 'NB'], 'Accuracy' : [knn_accuracy_withoutundersample*100,logreg_accuracy_withoutundersample*100,naivebayes_accuracy_withoutundersample*100]}

df= pd.DataFrame(accuracy)

ax= df.plot(kind='bar',x='Model',y='Accuracy',color='red', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Accuracy Comparison of the Models before Under Sampling')
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy')
plt.show()

# With Undersampling

accuracy2 = {'Model': ['KNN', 'LR', 'NB'], 'Accuracy': [knn_accuracy1*100, logreg_accuracy*100, nb_accuracy1*100]}
df = pd.DataFrame(accuracy2)
ax = df.plot(kind='bar', x='Model', y='Accuracy', color='purple', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Accuracy Comparison of Three Models')
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy')
plt.show()

# under sampling kora kharap karon amder accuracy onk kome jasche. without sampling kora amder model er jonno vlo  karon amra accuracy vlo pacchi and amder ml model e eta vlo  vabe fit kore

dict1 = {'KNN':knn_accuracy_withoutundersample*100, 'LR':logreg_accuracy_withoutundersample*100, 'NB':naivebayes_accuracy_withoutundersample*100}

dict2 = {'KNN':knn_accuracy1*100, 'LR':logreg_accuracy*100, 'NB':nb_accuracy1*100}


keys = list(dict1.keys())

values_dict1 = list(dict1.values())
values_dict2 = list(dict2.values())


plt.bar(keys, values_dict1, label='Before Under Sample', color='red')

plt.bar(keys, values_dict2, label='After Under Sample', color='purple')

plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Comparison of Three Models (Before & After Under Sampling)')

plt.legend()   # legend diye kon clr ki mean korche seta bujhacchi
plt.show()

















# pandas (pd): Used for data manipulation and analysis.
# numpy (np): Provides support for numerical computing with arrays and matrices.
# random: Offers tools for random number generation.
# scikit-learn (sklearn):
# SimpleImputer: For imputing missing values in datasets.
# train_test_split: Splits data into training and testing sets for model evaluation.
# MinMaxScaler: Scales features to a specified range (usually between 0 and 1).
# # StandardScaler: Standardizes features by removing the mean and scaling to unit variance.  bujhi nai
# # LabelEncoder: Encodes categorical integer features into numerical labels.
# # load_wine: Loads the wine dataset from scikit-learn datasets.        bujhai nai
# KNeighborsClassifier: Implements the k-nearest neighbors classifier algorithm.
# # confusion_matrix: Computes a confusion matrix for evaluating classification performance.   bujhi nai
# LogisticRegression: Implements logistic regression for classification tasks.
# GaussianNB: Implements Gaussian Naive Bayes classifier for classification tasks.
# # make_classification: Generates synthetic classification datasets.      bujhi nai
# accuracy_score: Computes the accuracy classification score.
# seaborn (sns): Used for data visualization and statistical graphics.
# plotly.express (px): Provides an easy-to-use interface for creating interactive plots.
# matplotlib.pyplot (plt): Another library for creating static, interactive, and animated visualizations in Python.